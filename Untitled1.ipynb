{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1c876a-21e2-4ccd-ae0c-bfa460efed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52849948-dbb8-40f4-a17e-c338bd0cbd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\Shubham Yadav\\Desktop\\Minor Project')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdaeb019-4045-4aba-881e-a4570b267259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to C:\\Users\\Shubham\n",
      "[nltk_data]     Yadav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6446e8-f3b7-4cc4-af4b-5964fd797c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv('balanced_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa475620-35e0-4eef-8044-86f78a0b2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Review': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c735eaa-7e64-4ac4-98a7-f3ccbd6a0215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Rate', 'text', 'Summary', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92df0c73-f8d3-4172-896d-6bb14a31ec0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rate</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44890</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Very poor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20163</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43142</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Does the job</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48690</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40956</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Decent product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38542</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Expected a better product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10917</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19833</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52799</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Bad quality</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15511</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9821</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46705</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Hated it!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54986</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10225</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24611</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43917</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Overpriced</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15381</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Utterly Disappointed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55010</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rate                       text  sentiment\n",
       "44890   1.0                  Very poor          0\n",
       "20163   5.0                   Terrific          1\n",
       "43142   3.0               Does the job          0\n",
       "48690   1.0                        Nan          0\n",
       "40956   3.0             Decent product          0\n",
       "38542   2.0  Expected a better product          0\n",
       "10917   5.0                  Just wow!          1\n",
       "19833   5.0                  Brilliant          1\n",
       "52799   2.0                Bad quality          0\n",
       "15511   5.0                  Brilliant          1\n",
       "9821    4.0                  Very Good          1\n",
       "46705   1.0                  Hated it!          0\n",
       "54986   3.0                        Nan          0\n",
       "10225   5.0        Best in the market!          1\n",
       "24611   5.0                  Fabulous!          1\n",
       "43917   2.0                 Overpriced          0\n",
       "2359    4.0               Nice product          1\n",
       "15381   5.0                  Brilliant          1\n",
       "36809   1.0       Utterly Disappointed          0\n",
       "55010   3.0                        Nan          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=[\n",
    "    'Summary'],\n",
    "        inplace=True)\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20804eaf-7974-4a32-b2d4-f7d09a5ab091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c0e8c3-ba26-4335-88e6-a81f5302f6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2367e5-0090-44ee-9cec-82313cee9fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7fdbc2-ddf7-4fcb-ba21-720d5e60ffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.26.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubham yadav\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45600274-ecf8-4c5d-9ec7-5dbe6e2335a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', finetuning_task='binary')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec575e7f-8ba4-48da-a51d-e9ec06062af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "def get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n",
    "  input_ids = tokenizer.encode(text, \n",
    "                               add_special_tokens=add_special_tokens, \n",
    "                               max_length=max_seq_length,\n",
    "                               pad_to_max_length=True)\n",
    "  attention_mask = [int(id > 0) for id in input_ids]\n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(attention_mask) == max_seq_length\n",
    "  return (input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e079c669-6c3d-48d8-ae61-69195b454910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a really good movie.\n",
      "Input Tokens: ['[CLS]', 'this', 'is', 'a', 'really', 'good', 'movie', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Input IDs: [101, 2023, 2003, 1037, 2428, 2204, 3185, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input text\n",
    "text = \"This is a really good movie.\"\n",
    "\n",
    "# Tokenize and prepare input IDs and attention mask\n",
    "tokenized_output = tokenizer(\n",
    "    text,\n",
    "    max_length=30,\n",
    "    padding=\"max_length\",  # Pads to the maximum length\n",
    "    truncation=True,       # Truncates if text is longer than max_length\n",
    "    return_tensors=\"pt\",   # Returns PyTorch tensors\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "input_ids = tokenized_output[\"input_ids\"][0].tolist()\n",
    "attention_mask = tokenized_output[\"attention_mask\"][0].tolist()\n",
    "\n",
    "# Convert input IDs back to tokens\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Input Tokens:\", input_tokens)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "612a2358-b912-4a07-a13d-b1e2f521e6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Tokenized Training Samples: 1284     [101, 2465, 2100, 4031, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "29071         [101, 27547, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "33960          [101, 4189, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3004           [101, 6919, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "41009     [101, 6283, 2009, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Name: text, dtype: object\n",
      "First 5 Tokenized Testing Samples: 10121          [101, 3492, 2204, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "36783          [101, 2025, 2204, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "35047    [101, 2025, 6749, 2012, 2035, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "11742            [101, 16660, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "15821            [101, 16660, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Handle missing or invalid values in the DataFrame\n",
    "df['text'] = df['text'].fillna(\"\").astype(str)  # Replace NaN with an empty string\n",
    "\n",
    "# Ensure 'sentiment' column is valid\n",
    "if df['sentiment'].isnull().any():\n",
    "    raise ValueError(\"The 'sentiment' column contains NaN values.\")\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df['text'],\n",
    "    df['sentiment'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['sentiment']  # Ensure balanced splitting based on sentiment\n",
    ")\n",
    "\n",
    "# Define a function to tokenize text\n",
    "def tokenize_text(text, tokenizer, max_length):\n",
    "    tokenized_output = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=None,  # Return plain lists, not tensors\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    return tokenized_output['input_ids']\n",
    "\n",
    "# Tokenize the training and testing datasets\n",
    "X_train_tokens = X_train.apply(lambda x: tokenize_text(x, tokenizer, 50))\n",
    "X_test_tokens = X_test.apply(lambda x: tokenize_text(x, tokenizer, 50))\n",
    "\n",
    "# Verify output\n",
    "print(\"First 5 Tokenized Training Samples:\", X_train_tokens.head())\n",
    "print(\"First 5 Tokenized Testing Samples:\", X_test_tokens.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aba8b0f-202a-4396-aadf-0e25119d8548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in X_train]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e727e89-a543-437d-aee9-96ea1391aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19487])\n",
      "torch.Size([19487])\n",
      "torch.Size([19487])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "input_ids_train = torch.tensor(\n",
    "    [features[0] for features in X_train_tokens.values], dtype=torch.long)\n",
    "input_mask_train = torch.tensor(\n",
    "    [features[1] for features in X_train_tokens.values], dtype=torch.long)\n",
    "label_ids_train = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "\n",
    "print (input_ids_train.shape)\n",
    "print (input_mask_train.shape)\n",
    "print (label_ids_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b545d83-005f-420a-8d7d-5e5ca33218b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(101)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5af0656-30d5-4067-99f6-ed80bed50967",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_train,input_mask_train,label_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e3a7c8e-8f33-4c33-852e-f8b4d26713e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = torch.tensor([features[0] for features in X_test_tokens.values], \n",
    "                              dtype=torch.long)\n",
    "input_mask_test = torch.tensor([features[1] for features in X_test_tokens.values], \n",
    "                               dtype=torch.long)\n",
    "label_ids_test = torch.tensor(Y_test.values, \n",
    "                              dtype=torch.long)\n",
    "test_dataset = TensorDataset(input_ids_test, input_mask_test, label_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "560b7856-d77f-4438-a9d4-4837ace69160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples =  19487\n",
      "Num Epochs =  2\n",
      "Total train batch size  =  64\n",
      "Total optimization steps =  152\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "train_batch_size = 64\n",
    "num_train_epochs = 2\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              sampler=train_sampler, \n",
    "                              batch_size=train_batch_size)\n",
    "t_total = len(train_dataloader) // num_train_epochs\n",
    "\n",
    "print (\"Num examples = \", len(train_dataset))\n",
    "print (\"Num Epochs = \", num_train_epochs)\n",
    "print (\"Total train batch size  = \", train_batch_size)\n",
    "print (\"Total optimization steps = \", t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0998ff02-cb54-43d3-8fc9-ebdc53f09ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, optimizer, and scheduler set up successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Define a dummy dataset for demonstration (replace with your actual dataset)\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Define tokenizer and dummy data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "texts = [\"Example sentence 1\", \"Example sentence 2\"]  # Replace with your text data\n",
    "labels = [0, 1]  # Replace with your labels\n",
    "dataset = DummyDataset(tokenizer, texts, labels)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-4\n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "num_epochs = 3\n",
    "\n",
    "# Calculate total training steps\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
    "\n",
    "# Define scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Model, optimizer, and scheduler set up successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed80624d-aed2-4ec6-aa03-4fbff605eae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f82e220bd6944668a4acc8457ecedaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|███████████████▍                                                             | 1/5 [00:00<00:00,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.413805"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb02f166397a4ec686d3cf706c41450a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.419171"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adc6eb99d294604a746ca87116e4683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████████████████████████████████████████████▏                              | 3/5 [00:00<00:00,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.420953"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3283b9ff7c64df682915936af4c7185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.421409"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a7fe663a7a45ab8a728196fd24fa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.421261Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import trange, notebook\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dummy Dataset (Replace with your actual dataset)\n",
    "input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "attention_mask = torch.tensor([[1, 1, 1], [1, 1, 1]])\n",
    "labels = torch.tensor([0, 1])\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Dummy Model (Replace with your actual model)\n",
    "model = torch.nn.Linear(3, 2)  # Example model with 3 inputs and 2 outputs\n",
    "model.to(device)  # Move model to GPU/CPU\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "# Training parameters\n",
    "num_train_epochs = 5\n",
    "\n",
    "# Training Loop\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\")\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = notebook.tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    \n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        # Handle various batch formats\n",
    "        if isinstance(batch, dict):  # If batch is a dictionary\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'],\n",
    "                'attention_mask': batch['attention_mask'],\n",
    "                'labels': batch['labels']\n",
    "            }\n",
    "        elif isinstance(batch, tuple):  # If batch is a tuple\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "        elif isinstance(batch, list):  # If batch is a list\n",
    "            batch = tuple(torch.stack(b).to(device) if isinstance(b, list) else b.to(device) for b in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected batch format: \", type(batch))\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs['input_ids'].float())  # Ensure float type if needed\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, inputs['labels'])  # Example loss function\n",
    "        \n",
    "        # Print loss for debugging\n",
    "        print(\"\\rLoss: %f\" % loss.item(), end='')\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Optimizer and scheduler step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66108156-6657-46e8-a75f-e0cec4f48c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to 'outputs' directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import os\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer to the 'outputs' directory\n",
    "model.save_pretrained(\"outputs\")\n",
    "tokenizer.save_pretrained(\"outputs\")\n",
    "\n",
    "print(\"Model and tokenizer saved to 'outputs' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e1d7f-4f43-4aa0-8693-480d30bd4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler, Dataset\n",
    "from transformers import BertForSequenceClassification, default_data_collator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "test_batch_size = 64\n",
    "model_path = r'C:\\Users\\Shubham Yadav\\Desktop\\Minor Project\\outputs'\n",
    "\n",
    "# Example Test Dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, inputs, masks, labels):\n",
    "        self.inputs = inputs\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx],\n",
    "            \"attention_mask\": self.masks[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# Replace these tensors with your actual test data\n",
    "test_inputs = torch.randint(0, 100, (1000, 128))  # Example input_ids\n",
    "test_masks = torch.ones(1000, 128)  # Example attention_mask\n",
    "test_labels = torch.randint(0, 2, (1000,))  # Example labels\n",
    "\n",
    "test_dataset = TestDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoader\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=test_sampler,\n",
    "    batch_size=test_batch_size,\n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize variables for predictions and true labels\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate through batches\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    # Send batch to device\n",
    "    batch = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "\n",
    "    # Extract loss and logits\n",
    "    _, logits = outputs[:2]\n",
    "\n",
    "    # Accumulate predictions and labels\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = batch[\"labels\"].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, batch[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "# Compute final predictions and accuracy\n",
    "preds = np.argmax(preds, axis=1)\n",
    "acc_score = accuracy_score(out_label_ids, preds)\n",
    "\n",
    "print(f'Accuracy Score on Test data: {acc_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14de11-dada-470f-bcf7-f426d587ee70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
